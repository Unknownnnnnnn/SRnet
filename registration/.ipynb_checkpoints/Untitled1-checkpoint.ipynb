{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# ---- Multi-Head Attention ----\n",
    "# ------------------------------\n",
    "\n",
    "class MHAtt(nn.Module):\n",
    "    def __init__(self, __C):\n",
    "        super(MHAtt, self).__init__()\n",
    "        self.__C = __C\n",
    "\n",
    "        self.linear_v = nn.Linear(__C['HIDDEN_SIZE'], __C['HIDDEN_SIZE'])\n",
    "        self.linear_k = nn.Linear(__C['HIDDEN_SIZE'], __C['HIDDEN_SIZE'])\n",
    "        self.linear_q = nn.Linear(__C['HIDDEN_SIZE'], __C['HIDDEN_SIZE'])\n",
    "        self.linear_merge = nn.Linear(__C['HIDDEN_SIZE'], __C['HIDDEN_SIZE'])\n",
    "\n",
    "        self.dropout = nn.Dropout(__C['DROPOUT_R'])\n",
    "\n",
    "    def forward(self, v, k, q, mask):\n",
    "        n_batches = q.size(0)\n",
    "\n",
    "        v = self.linear_v(v).view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.__C['MULTI_HEAD'],\n",
    "            self.__C['HIDDEN_SIZE_HEAD']\n",
    "        ).transpose(1, 2)  # b, head, seq, hidden_dim\n",
    "\n",
    "        k = self.linear_k(k).view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.__C['MULTI_HEAD'],\n",
    "            self.__C['HIDDEN_SIZE_HEAD']\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        q = self.linear_q(q).view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.__C['MULTI_HEAD'],\n",
    "            self.__C['HIDDEN_SIZE_HEAD']\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        atted = self.att(v, k, q, mask)\n",
    "        atted = atted.transpose(1, 2).contiguous().view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.__C['HIDDEN_SIZE']\n",
    "        )\n",
    "\n",
    "        atted = self.linear_merge(atted)\n",
    "\n",
    "        return atted\n",
    "\n",
    "    def att(self, value, key, query, mask):\n",
    "        d_k = query.size(-1) # hidden dim\n",
    "\n",
    "        scores = torch.matmul(\n",
    "            query, key.transpose(-2, -1)\n",
    "        ) / math.sqrt(d_k) # (b, head, seq_q, hidden_dim) x (b, head, hidden_dim, seq_k) -> (b,head,seq_q,seq_k)\n",
    "\n",
    "        if mask is not None: # mask(b, seq_q)\n",
    "            scores = scores.masked_fill(mask, -1e9)  # value 中 padding部分会\n",
    "\n",
    "        att_map = F.softmax(scores, dim=-1)  # query中每个词 在 所有value上的概率分布\n",
    "        att_map = self.dropout(att_map)\n",
    "\n",
    "        return torch.matmul(att_map, value) # (b,head,seq_q,seq_k) x (b, head, seq_k, hidden_)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# ---- Feed Forward Nets ----\n",
    "# ---------------------------\n",
    "\n",
    "class FFN(nn.Module):\n",
    "    def __init__(self, __C):\n",
    "        super(FFN, self).__init__()\n",
    "\n",
    "        self.mlp = MLP(\n",
    "            in_size=__C['HIDDEN_SIZE'],\n",
    "            mid_size=__C['FF_SIZE'],\n",
    "            out_size=__C['HIDDEN_SIZE'],\n",
    "            dropout_r=__C['DROPOUT_R'],\n",
    "            use_relu=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "# ---- Self Attention ----\n",
    "# ------------------------\n",
    "\n",
    "class SA(nn.Module):\n",
    "    def __init__(self, __C):\n",
    "        super(SA, self).__init__()\n",
    "\n",
    "        self.mhatt = MHAtt(__C)\n",
    "        self.ffn = FFN(__C)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(__C['DROPOUT_R'])\n",
    "        self.norm1 = LayerNorm(__C['HIDDEN_SIZE'])\n",
    "\n",
    "        self.dropout2 = nn.Dropout(__C['DROPOUT_R'])\n",
    "        self.norm2 = LayerNorm(__C['HIDDEN_SIZE'])\n",
    "\n",
    "    def forward(self, x, x_mask):\n",
    "        x = self.norm1(x + self.dropout1(\n",
    "            self.mhatt(x, x, x, x_mask)\n",
    "        )) # (b, seq_q, hidden_dim)\n",
    "\n",
    "        x = self.norm2(x + self.dropout2(\n",
    "            self.ffn(x)\n",
    "        ))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# ---- Self Guided Attention ----\n",
    "# -------------------------------\n",
    "class SGA(nn.Module):\n",
    "    def __init__(self, __C):\n",
    "        super(SGA, self).__init__()\n",
    "\n",
    "        self.mhatt1 = MHAtt(__C)\n",
    "        self.mhatt2 = MHAtt(__C)\n",
    "        self.ffn = FFN(__C)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(__C['DROPOUT_R'])\n",
    "        self.norm1 = LayerNorm(__C['HIDDEN_SIZE'])\n",
    "\n",
    "        self.dropout2 = nn.Dropout(__C['DROPOUT_R'])\n",
    "        self.norm2 = LayerNorm(__C['HIDDEN_SIZE'])\n",
    "\n",
    "        self.dropout3 = nn.Dropout(__C['DROPOUT_R'])\n",
    "        self.norm3 = LayerNorm(__C['HIDDEN_SIZE'])\n",
    "\n",
    "    def forward(self, x, y, x_mask, y_mask):\n",
    "        x = self.norm1(x + self.dropout1(\n",
    "            self.mhatt1(x, x, x, x_mask)\n",
    "        ))\n",
    "\n",
    "        x = self.norm2(x + self.dropout2(\n",
    "            self.mhatt2(y, y, x, y_mask)\n",
    "        ))\n",
    "\n",
    "        x = self.norm3(x + self.dropout3(\n",
    "            self.ffn(x)\n",
    "        ))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SGA_last(nn.Module):\n",
    "    def __init__(self, __C):\n",
    "        super(SGA_last, self).__init__()\n",
    "\n",
    "        self.mhatt1 = MHAtt(__C)\n",
    "        self.mhatt2 = MHAtt(__C)\n",
    "        self.ffn = FFN(__C)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(__C['DROPOUT_R'])\n",
    "        self.norm1 = LayerNorm(__C['HIDDEN_SIZE'])\n",
    "\n",
    "        self.dropout2 = nn.Dropout(__C['DROPOUT_R'])\n",
    "        self.norm2 = LayerNorm(__C['HIDDEN_SIZE'])\n",
    "\n",
    "        self.dropout3 = nn.Dropout(__C['DROPOUT_R'])\n",
    "        self.norm3 = LayerNorm(__C['HIDDEN_SIZE'])\n",
    "\n",
    "    def forward(self, x, y, x_mask, y_mask):\n",
    "        x = self.norm1(x + self.dropout1(\n",
    "            self.mhatt1(x, x, x, x_mask)\n",
    "        ))\n",
    "\n",
    "        # x = self.norm2(x + self.dropout2(\n",
    "        #     self.mhatt2(y, y, x, y_mask)\n",
    "        # ))\n",
    "\n",
    "        x = self.norm2(self.dropout2(\n",
    "            self.mhatt2(x, x, y, x_mask)\n",
    "        ))\n",
    "\n",
    "        x = self.norm3(self.dropout3(\n",
    "            self.ffn(x)\n",
    "        ))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# ------------------------------------------------\n",
    "# ---- MAC Layers Cascaded by Encoder-Decoder ----\n",
    "# ------------------------------------------------\n",
    "\n",
    "class MCA_ED(nn.Module):\n",
    "    def __init__(self, __C):\n",
    "        super(MCA_ED, self).__init__()\n",
    "\n",
    "        self.enc_list = nn.ModuleList([SA(__C) for _ in range(__C['LAYER'])])\n",
    "        self.dec_list = nn.ModuleList([SGA(__C) for _ in range(__C['LAYER'] - 1)])\n",
    "\n",
    "        self.dec_last = SGA_last(__C)\n",
    "\n",
    "    def forward(self, x, y, x_mask, y_mask):\n",
    "        # Get hidden vector\n",
    "        for enc in self.enc_list:\n",
    "            x = enc(x, x_mask)\n",
    "\n",
    "        for dec in self.dec_list:\n",
    "            y = dec(y, x, y_mask, x_mask)\n",
    "\n",
    "        y = self.dec_last(y, x, y_mask, x_mask)\n",
    "        return x, y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppp",
   "language": "python",
   "name": "ppp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
